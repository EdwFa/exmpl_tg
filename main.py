import streamlit as st
import yaml
import json
import os
from openai import OpenAI
import tiktoken

# === –ü–£–¢–ò –ò –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–Ø ===

TOGETHER_API_KEY = st.secrets.get("TOGETHER_API_KEY") or "your-together-api-key-here"
PROMPTS_DIR = "prompts"

CHAT_MODELS = {
    "Qwen/Qwen3-Next-80B-A3B-Instruct": "Qwen3 Next 80B",
    "meta-llama/Llama-3-70b-chat-hf": "Llama 3 70B Chat",
    "meta-llama/Llama-3-8b-chat-hf": "Llama 3 8B Chat",
    "mistralai/Mixtral-8x7B-Instruct-v0.1": "Mixtral 8x7B Instruct",
    "google/gemma-2-9b-it": "Gemma 2 9B Instruct",
}

MODEL_CONTEXT_LIMITS = {
    "Qwen/Qwen3-Next-80B-A3B-Instruct": 32768,
    "meta-llama/Llama-3-70b-chat-hf": 8192,
    "meta-llama/Llama-3-8b-chat-hf": 8192,
    "mistralai/Mixtral-8x7B-Instruct-v0.1": 32768,
    "google/gemma-2-9b-it": 8192,
}

DEFAULT_PROMPT_NAME = "default"

def count_tokens(text: str, model: str = "gpt-3.5-turbo") -> int:
    try:
        encoding = tiktoken.encoding_for_model(model)
        return len(encoding.encode(text))
    except:
        return len(text) // 4

def trim_messages(messages, system_prompt, model_id, max_gen_tokens=1024):
    context_limit = MODEL_CONTEXT_LIMITS.get(model_id, 4096)
    available_tokens = context_limit - max_gen_tokens - 200
    total_tokens = count_tokens(system_prompt, model_id)
    trimmed = [{"role": "system", "content": system_prompt}]
    for msg in reversed(messages[1:]):
        msg_tokens = count_tokens(msg["content"], model_id)
        if total_tokens + msg_tokens > available_tokens:
            break
        trimmed.insert(1, msg)
        total_tokens += msg_tokens
    if len(trimmed) == 1 and len(messages) > 1:
        trimmed.append(messages[-1])
    return trimmed

# === –ó–ê–ì–†–£–ó–ö–ê –ü–†–û–ú–ü–¢–û–í –ò–ó –§–ê–ô–õ–û–í ===

def load_prompt_from_path(filepath):
    if not os.path.exists(filepath):
        return None
    try:
        with open(filepath, "r", encoding="utf-8") as f:
            if filepath.endswith((".yaml", ".yml")):
                data = yaml.safe_load(f)
            elif filepath.endswith(".json"):
                data = json.load(f)
            else:
                return None
        if isinstance(data, dict) and "system_prompt" in data:
            return data["system_prompt"].strip()
    except Exception as e:
        st.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ {filepath}: {e}")
    return None

def get_available_prompt_profiles():
    profiles = []
    if os.path.exists(PROMPTS_DIR):
        for f in sorted(os.listdir(PROMPTS_DIR)):
            if f.endswith((".yaml", ".yml", ".json")):
                name = os.path.splitext(f)[0]
                profiles.append(name)
    return profiles if profiles else [DEFAULT_PROMPT_NAME]

# === –ò–ù–¢–ï–†–§–ï–ô–° ===

st.set_page_config(page_title="üí¨ –ß–∞—Ç —Å –ø—Ä–æ—Ñ–∏–ª—è–º–∏ –ø—Ä–æ–º–ø—Ç–æ–≤", layout="wide")
st.title("üí¨ –ò–ò-–∞—Å—Å–∏—Å—Ç–µ–Ω—Ç —Å –ø—Ä–æ—Ñ–∏–ª—è–º–∏ (Together.ai)")

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å–æ—Å—Ç–æ—è–Ω–∏—è
if "messages" not in st.session_state:
    st.session_state.messages = []
if "current_system_prompt" not in st.session_state:
    st.session_state.current_system_prompt = ""
if "model_id" not in st.session_state:
    st.session_state.model_id = list(CHAT_MODELS.keys())[0]
if "selected_profile" not in st.session_state:
    st.session_state.selected_profile = DEFAULT_PROMPT_NAME
if "uploaded_prompt" not in st.session_state:
    st.session_state.uploaded_prompt = None

# –ü–æ–ª—É—á–∞–µ–º —Å–ø–∏—Å–æ–∫ –ø—Ä–æ—Ñ–∏–ª–µ–π
available_profiles = get_available_prompt_profiles()

# –ë–æ–∫–æ–≤–∞—è –ø–∞–Ω–µ–ª—å
with st.sidebar:
    st.header("‚öôÔ∏è –ü—Ä–æ–º–ø—Ç—ã –∏ –º–æ–¥–µ–ª–∏")

    # –í—ã–±–æ—Ä –ø—Ä–æ—Ñ–∏–ª—è
    selected = st.selectbox(
        "üìÅ –í—ã–±–µ—Ä–∏—Ç–µ –ø—Ä–æ—Ñ–∏–ª—å –ø—Ä–æ–º–ø—Ç–∞",
        options=available_profiles,
        index=available_profiles.index(st.session_state.selected_profile)
        if st.session_state.selected_profile in available_profiles
        else 0
    )
    st.session_state.selected_profile = selected

    # –ó–∞–≥—Ä—É–∑–∫–∞ —Ñ–∞–π–ª–∞ (–ø–µ—Ä–µ–æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç –ø—Ä–æ—Ñ–∏–ª—å)
    prompt_file = st.file_uploader("üì§ –ò–ª–∏ –∑–∞–≥—Ä—É–∑–∏—Ç–µ —Å–≤–æ–π —Ñ–∞–π–ª (YAML/JSON)", type=["yaml", "yml", "json"])
    if prompt_file:
        st.session_state.uploaded_prompt = prompt_file
    else:
        st.session_state.uploaded_prompt = None

    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∞–∫—Ç—É–∞–ª—å–Ω—ã–π —Å–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç
    if st.session_state.uploaded_prompt:
        # –ó–∞–≥—Ä—É–∂–µ–Ω–Ω—ã–π —Ñ–∞–π–ª –∏–º–µ–µ—Ç –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç
        file = st.session_state.uploaded_prompt
        try:
            content = file.read().decode("utf-8")
            if file.name.endswith((".yaml", ".yml")):
                data = yaml.safe_load(content)
            else:
                data = json.loads(content)
            system_prompt = data.get("system_prompt", "").strip() if isinstance(data, dict) else ""
        except:
            system_prompt = ""
        if not system_prompt:
            st.warning("–§–∞–π–ª –Ω–µ —Å–æ–¥–µ—Ä–∂–∏—Ç system_prompt. –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –ø—Ä–æ—Ñ–∏–ª—å.")
            st.session_state.uploaded_prompt = None
    else:
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä–æ—Ñ–∏–ª—å
        filepath = os.path.join(PROMPTS_DIR, f"{st.session_state.selected_profile}.yaml")
        if not os.path.exists(filepath):
            filepath = os.path.join(PROMPTS_DIR, f"{st.session_state.selected_profile}.json")
        system_prompt = load_prompt_from_path(filepath) or ""

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ç–µ–∫—É—â–∏–π –ø—Ä–æ–º–ø—Ç
    if system_prompt != st.session_state.current_system_prompt:
        st.session_state.current_system_prompt = system_prompt
        # –û–±–Ω–æ–≤–ª—è–µ–º —Å–∏—Å—Ç–µ–º–Ω–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ
        if st.session_state.messages and st.session_state.messages[0]["role"] == "system":
            st.session_state.messages[0]["content"] = system_prompt
        else:
            st.session_state.messages.insert(0, {"role": "system", "content": system_prompt})

    st.text_area("–¢–µ–∫—É—â–∏–π —Å–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç", value=st.session_state.current_system_prompt, height=150, disabled=True)

    # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –º–æ–¥–µ–ª–∏
    api_key = st.text_input("üîë API-–∫–ª—é—á Together.ai", type="password", value=TOGETHER_API_KEY)
    st.session_state.api_key = api_key

    model_id = st.selectbox(
        "üß† –ú–æ–¥–µ–ª—å",
        options=list(CHAT_MODELS.keys()),
        format_func=lambda x: CHAT_MODELS[x],
        index=0
    )
    st.session_state.model_id = model_id

    temperature = st.slider("üå°Ô∏è Temperature", 0.0, 1.0, 0.3, 0.05)
    max_tokens = st.slider("üìè –ú–∞–∫—Å. —Ç–æ–∫–µ–Ω–æ–≤ –≤ –æ—Ç–≤–µ—Ç–µ", 128, 4096, 1024, 128)
    top_p = st.slider("üéØ Top-p", 0.1, 1.0, 0.9, 0.05)

    if st.button("üóëÔ∏è –û—á–∏—Å—Ç–∏—Ç—å —á–∞—Ç"):
        st.session_state.messages = [{"role": "system", "content": st.session_state.current_system_prompt}]
        st.rerun()

# –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–ª—é—á–∞
if not st.session_state.api_key:
    st.warning("–í–≤–µ–¥–∏—Ç–µ API-–∫–ª—é—á Together.ai.")
    st.stop()

if not st.session_state.current_system_prompt:
    st.warning("–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å —Å–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ —Ñ–∞–π–ª—ã –≤ –ø–∞–ø–∫–µ prompts/.")

# –û—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ –∏—Å—Ç–æ—Ä–∏–∏ (–±–µ–∑ —Å–∏—Å—Ç–µ–º–Ω–æ–≥–æ —Å–æ–æ–±—â–µ–Ω–∏—è)
for msg in st.session_state.messages[1:]:
    role = "user" if msg["role"] == "user" else "assistant"
    with st.chat_message(role):
        st.markdown(msg["content"])

# –í–≤–æ–¥ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
if prompt := st.chat_input("–í–∞—à –≤–æ–ø—Ä–æ—Å..."):
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    messages_to_send = trim_messages(
        st.session_state.messages,
        st.session_state.current_system_prompt,
        st.session_state.model_id,
        max_tokens
    )

    with st.chat_message("assistant"):
        with st.spinner("–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞..."):
            try:
                client = OpenAI(
                    api_key=st.session_state.api_key,
                    base_url="https://api.together.xyz/v1"
                )
                response = client.chat.completions.create(
                    model=st.session_state.model_id,
                    messages=messages_to_send,
                    max_tokens=max_tokens,
                    temperature=temperature,
                    top_p=top_p,
                    stream=False
                )
                reply = response.choices[0].message.content.strip()
                st.markdown(reply)
                st.session_state.messages.append({"role": "assistant", "content": reply})
            except Exception as e:
                st.error(f"–û—à–∏–±–∫–∞: {e}")